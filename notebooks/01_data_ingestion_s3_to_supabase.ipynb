{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngsQr4s6N0qg"
      },
      "source": [
        "# Data Ingestion: S3 → Supabase (PostgreSQL)\n",
        "\n",
        "Este notebook faz parte de um projeto de engenharia de dados que simula um pipeline de ingestão batch\n",
        "para um e-commerce.\n",
        "\n",
        "## Objetivo\n",
        "- Ler dados armazenados no S3 (Supabase Storage)\n",
        "- Suportar múltiplos formatos (CSV e Parquet)\n",
        "- Carregar os dados em um banco PostgreSQL (Supabase)\n",
        "- Garantir um padrão de ingestão reutilizável\n",
        "\n",
        "## Datasets\n",
        "- clientes.csv\n",
        "- produtos.csv\n",
        "- vendas.csv\n",
        "- preco_competidores.parquet\n",
        "\n",
        "Este notebook foca **exclusivamente na ingestão dos dados**.\n",
        "Transformações e modelagem analítica serão tratadas em etapas posteriores (dbt).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7su2lzeSdp2"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Instalação de dependências (Colab)\n",
        "# =========================\n",
        "\n",
        "!pip install boto3 sqlalchemy pyarrow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7rBVXaZJNgMt"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Setup & Imports\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import io\n",
        "\n",
        "import pandas as pd\n",
        "import boto3\n",
        "from sqlalchemy import create_engine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NFkboH4aOYyF"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Configurações\n",
        "# =========================\n",
        "\n",
        "# S3 / Supabase Storage\n",
        "S3_ENDPOINT_URL = os.getenv(\"S3_ENDPOINT_URL\")\n",
        "S3_REGION = os.getenv(\"S3_REGION\", \"us-east-1\")\n",
        "S3_ACCESS_KEY = os.getenv(\"S3_ACCESS_KEY\")\n",
        "S3_SECRET_KEY = os.getenv(\"S3_SECRET_KEY\")\n",
        "S3_BUCKET_NAME = os.getenv(\"S3_BUCKET_NAME\")\n",
        "\n",
        "# Banco de Dados (Supabase - PostgreSQL)\n",
        "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Jkt0tJs1OjpI"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Conexões\n",
        "# =========================\n",
        "\n",
        "# Client S3 (Supabase Storage via S3 API)\n",
        "s3_client = boto3.client(\n",
        "    \"s3\",\n",
        "    endpoint_url=S3_ENDPOINT_URL,\n",
        "    region_name=S3_REGION,\n",
        "    aws_access_key_id=S3_ACCESS_KEY,\n",
        "    aws_secret_access_key=S3_SECRET_KEY,\n",
        ")\n",
        "\n",
        "# Engine PostgreSQL (Supabase)\n",
        "engine = create_engine(DATABASE_URL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6wbhzNzMOy9a"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Funções de Ingestão\n",
        "# =========================\n",
        "\n",
        "def read_csv_from_s3(bucket: str, key: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Lê um arquivo CSV do S3 e retorna um DataFrame.\n",
        "    \"\"\"\n",
        "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
        "    return pd.read_csv(response[\"Body\"])\n",
        "\n",
        "\n",
        "def read_parquet_from_s3(bucket: str, key: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Lê um arquivo Parquet do S3 e retorna um DataFrame.\n",
        "    \"\"\"\n",
        "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
        "    return pd.read_parquet(io.BytesIO(response[\"Body\"].read()))\n",
        "\n",
        "\n",
        "def write_to_postgres(df: pd.DataFrame, table_name: str):\n",
        "    \"\"\"\n",
        "    Grava um DataFrame no PostgreSQL (Supabase).\n",
        "    \"\"\"\n",
        "    df.to_sql(\n",
        "        name=table_name,\n",
        "        con=engine,\n",
        "        if_exists=\"replace\",\n",
        "        index=False\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RAkxSMDPBHG",
        "outputId": "ad7d4fdb-840d-4b33-bbc5-706df89d3c87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "clientes carregada (50 linhas)\n",
            "produtos carregada (215 linhas)\n",
            "vendas carregada (3020 linhas)\n",
            "preco_competidores carregada (728 linhas)\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Ingestão dos Datasets\n",
        "# =========================\n",
        "\n",
        "datasets = [\n",
        "    {\n",
        "        \"type\": \"csv\",\n",
        "        \"key\": \"clientes.csv\",\n",
        "        \"table\": \"clientes\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"csv\",\n",
        "        \"key\": \"produtos.csv\",\n",
        "        \"table\": \"produtos\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"csv\",\n",
        "        \"key\": \"vendas.csv\",\n",
        "        \"table\": \"vendas\"\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"parquet\",\n",
        "        \"key\": \"preco_competidores.parquet\",\n",
        "        \"table\": \"preco_competidores\"\n",
        "    },\n",
        "]\n",
        "\n",
        "for dataset in datasets:\n",
        "    if dataset[\"type\"] == \"csv\":\n",
        "        df = read_csv_from_s3(S3_BUCKET_NAME, dataset[\"key\"])\n",
        "    else:\n",
        "        df = read_parquet_from_s3(S3_BUCKET_NAME, dataset[\"key\"])\n",
        "\n",
        "    write_to_postgres(df, dataset[\"table\"])\n",
        "\n",
        "    print(f\"{dataset['table']} carregada ({df.shape[0]} linhas)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5sslp2BPRBn",
        "outputId": "42ea712a-cd91-42a2-ef83-10cf577cfeef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tabela 'clientes': 50 registros\n",
            "Tabela 'produtos': 215 registros\n",
            "Tabela 'vendas': 3020 registros\n",
            "Tabela 'preco_competidores': 728 registros\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Validação da Ingestão\n",
        "# =========================\n",
        "\n",
        "tables = [\"clientes\", \"produtos\", \"vendas\", \"preco_competidores\"]\n",
        "\n",
        "for table in tables:\n",
        "    df_check = pd.read_sql(f\"SELECT COUNT(*) AS total FROM {table}\", engine)\n",
        "    total = df_check.loc[0, \"total\"]\n",
        "    print(f\"Tabela '{table}': {total} registros\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
